# -*- coding: utf-8 -*-
"""TechChallenge#4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xHyhS7xYwSHL1e_bUYWCJEwRyTxTdgQi
"""

!pip uninstall -y fbprophet
!pip uninstall -y pystan
!pip install fbprophet
!pip install pystan==2.19.1.1

# Commented out IPython magic to ensure Python compatibility.
# Bibliotecas b√°sicas de data science
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.graph_objects as go
# %matplotlib inline

# Para baixar os dados
import yfinance as yf


#Para aplicar o ARIMA:
from statsmodels.tsa.stattools import adfuller          #verificar dados estacion√°rios
from statsmodels.tsa.stattools import acf, pacf         #correla√ß√£o
from statsmodels.tsa.seasonal import seasonal_decompose #Decomposi√ß√£o sazonal usando m√©dias m√≥veis.
from statsmodels.tsa.arima.model import ARIMA           #algoritmo arima

# Para deep learning
from keras.models import Sequential
from keras.layers import LSTM,Dense,Dropout
from tensorflow.keras.optimizers import Adam
from keras.models import load_model
from keras.preprocessing.sequence import TimeseriesGenerator

# Para machine learning
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

import warnings
warnings.filterwarnings('ignore')

# Prophet
from prophet import Prophet

"""#Upload do DataFrame"""

import yfinance as yf

# Especifique o s√≠mbolo do Brent Crude Oil Last Day Financ (BZ=F) e o intervalo de datas desejado
symbol = 'BZ=F'

start_date = '1987-05-20' #data mais antiga do valor do IPEA (url providenciada no challenge)
end_date = '2024-05-20' #data escolhida para o final dos dados

# Use a fun√ß√£o download para obter os dados
df = yf.download(symbol, start=start_date, end=end_date)

df

df_copy = df

df = df.reset_index('Date')
df['Date'] = pd.to_datetime(df['Date']) #realizando a convers√£o da data para formato datetime
df.head()

df.shape #analisando linhas e colunas

df_brent = df # criando c√≥pia do dataset

"""### Visualizando os valores de fechamento"""

plt.figure(figsize = (15,10))
plt.plot(df['Date'], df['Close'], label='Brent Price')

plt.legend(loc='best')
plt.show()

from statsmodels.tsa.seasonal import seasonal_decompose
import matplotlib.pyplot as plt

df_limpo = df                                                              # c√≥pia do dataframe
df_limpo['Date'] = pd.to_datetime(df_limpo['Date'])                        # transformando em datetime
df_limpo = df.drop(columns=['Open', 'High', 'Low', 'Volume', 'Adj Close']) # Removendo colunas desnecess√°rias
df_limpo = df_limpo.set_index('Date')                                      # definindo data como √≠ndice

################################## Notas da an√°lise de decomposi√ß√£o da s√©rie ########################################

# model='additive': Este par√¢metro especifica o tipo de modelo utilizado na decomposi√ß√£o.

# Os dois tipos principais s√£o:

# "additive" (modelo apropriado quando a magnitude da sazonalidade n√£o varia com a tend√™ncia)
# "multiplicative" (modelo √© mais apropriado quando a magnitude da sazonalidade varia com a tend√™ncia).

# period: Este √© o per√≠odo da sazonalidade. Ele especifica o n√∫mero de observa√ß√µes em um ciclo sazonal.

######################################################################################################################

seasonplot = seasonal_decompose(df_limpo, model='multiplicative', period=7) #decopondo a s√©rie temporal
fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(15,8))
seasonplot.observed.plot(ax=ax1) #serie real
seasonplot.trend.plot(ax=ax2)    #tendencia
seasonplot.seasonal.plot(ax=ax3) #sazonalisade
seasonplot.resid.plot(ax=ax4)    #residuos
plt.tight_layout()

df_limpo

#Filtrando os dados
df = pd.DataFrame(df[['Close','Date']])
df.set_index('Date', inplace=True)

df['MA_window_10'] = df['Close'].rolling(10).mean().shift() #m√©dia m√≥vel em 10 dias
df['MA_window_20'] = df['Close'].rolling(20).mean().shift() #m√©dia m√≥vel em 20 dias

#Rolling para deslocamento. Este par√¢metro especifica o n√∫mero de observa√ß√µes usadas para calcular a estat√≠stica
#shift √© utilizado para deslocar o √≠ndice de DataFrame por um n√∫mero especificado de per√≠odos com uma freq√º√™ncia de tempo opcional

df.head(20)

"""### Visualizando o Resultado"""

plt.figure(figsize=(15,10))
plt.grid(True)
plt.plot(df['Close'], label='Close')
plt.plot(df['MA_window_10'], label='MA window 10 days')
plt.plot(df['MA_window_20'], label='MA window 20 days')
plt.legend(loc=2)
plt.show()

"""### Visualizando os dados mais recentes: √∫ltimos 300 dias"""

limit = 365

plt.figure(figsize=(15,10))
plt.grid(True)
plt.plot(df['Close'][-limit:], label='Close')
plt.plot(df['MA_window_10'][-limit:], label='MA window 10 days')
plt.plot(df['MA_window_20'][-limit:], label='MA window 20 days')
plt.legend(loc=2)
plt.show()

"""### Prevendo os "N" dias com base na m√©dia m√≥vel aritm√©tica"""

df['MA_window_10_forward_10'] = np.NaN #preechendo com NaN os valores da coluna de MA_window_10_forward_10

def make_window(window_size, start_point):
    return [start_point+x for x in range(window_size)]  #realizando a janela de tempo

window_size = 10
forward_days = 10

# Itera√ß√£o sobre a s√©rie temporal com uma janela deslizante
for index in range(window_size, len(df), forward_days):

    # Itera√ß√£o para calcular a m√©dia m√≥vel ponderada
    for i in range(0, forward_days):
        # Verifica se o √≠ndice atual est√° dentro dos limites da s√©rie temporal
        if index + i >= len(df):
            break

        # Criando duas janelas:
        # 1. Uma janela para a m√©dia m√≥vel ('window_close')
        # 2. Uma janela para a s√©rie temporal original ('window_MA')
        window_close = make_window(window_size - i, index + i - window_size)
        window_MA = make_window(i, index)

        # Calculando a m√©dia m√≥vel ponderada
        mean = pd.concat([df['Close'].iloc[window_close], df['MA_window_10_forward_10'].iloc[window_MA]]).mean(axis=0)

        # Atualizando o DataFrame com a m√©dia m√≥vel ponderada
        df.iat[index + i, df.columns.get_loc('MA_window_10_forward_10')] = mean

"""Plotando os dados"""

plt.figure(figsize = (15,10))

size = len(df)-limit - (len(df)-limit)%forward_days

for index in range(size, len(df), forward_days):
    plt.plot(df['MA_window_10_forward_10'][index:index+forward_days], color='r')

plt.plot(df['Close'][-limit:], color='b', label='Close')
#plt.legend(loc='best')
plt.show()

"""Testando ARIMA"""

import statsmodels.api as sm
sm.graphics.tsa.plot_acf(df_limpo, lags=50)
plt.show()

sm.graphics.tsa.plot_pacf(df_limpo, lags=50)
plt.show()

#Analisando M√©dia M√≥vel e Desvio Padr√£o

rolmean = df_limpo.rolling(window=12).mean() #m√©dia m√≥vel em 12 meses (dando a m√©dia m√≥vel no n√≠vel anual).
rolstd = df_limpo.rolling(window=12).std()   #Desvio padr√£o em 12 meses
print(rolmean,rolstd)

#Plotando a m√©dia m√≥vel
orig = plt.plot(df_limpo, color='blue', label='Original')
mean = plt.plot(rolmean, color='red', label='Rolling Mean')
std = plt.plot(rolstd, color='black', label='Rolling Std')
plt.legend(loc='best')
plt.title('M√©dia m√≥vel e desvio padr√£o')
plt.show(block=False)

"""# Verificar estacionariedade da s√©rie temporal üìà"""

from statsmodels.tsa.stattools import adfuller

# Teste de estacionariedade (ADF Test)
adf_result = adfuller(df_limpo['Close'])
print(f'ADF Statistic: {adf_result[0]}')
print(f'p-value: {adf_result[1]}')
print('Resultados do Teste de Estacionariedade:')
print('--------------------------------------')
print('Teste Estat√≠stico:', adf_result[0])
print('Valor-p:', adf_result[1])
print('Valores Cr√≠ticos:')
for key, value in adf_result[4].items():
    print(f'   {key}: {value}')

#Estimating trend
indexedDataset_logScale = np.log(df_limpo) #Transforma√ß√£o logar√≠tma
plt.plot(indexedDataset_logScale)

# Calculando a m√©dia m√≥vel
movingAverage = indexedDataset_logScale.rolling(window=12).mean()
movingSTD = indexedDataset_logScale.rolling(window=12).std()
plt.plot(indexedDataset_logScale)
plt.plot(movingAverage, color='red')

datasetLogScaleMinusMovingAverage = indexedDataset_logScale - movingAverage
datasetLogScaleMinusMovingAverage.head(12)

#Remove NAN values
datasetLogScaleMinusMovingAverage.dropna(inplace=True)
datasetLogScaleMinusMovingAverage.head(10)

# Fun√ß√£o para testar a estacionariedade da s√©rie transformada

def test_stationarity(timeseries):

    #Determinar estat√≠sticas cont√≠nuas
    movingAverage = timeseries.rolling(window=12).mean()
    movingSTD = timeseries.rolling(window=12).std()

    #Plot estat√≠sticas cont√≠nuas
    orig = plt.plot(timeseries, color='blue', label='Original')
    mean = plt.plot(movingAverage, color='red', label='Rolling Mean')
    std = plt.plot(movingSTD, color='black', label='Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)

    #Performance do Dickey‚ÄìFuller:
    print('Results of Dickey Fuller Test:')
    dftest = adfuller(timeseries['Close'], autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
    print(dfoutput)

test_stationarity(datasetLogScaleMinusMovingAverage)

"""### 2¬∫ teste : Decaimento exponencial.



---



**Objetivo:** Remover a tend√™ncia de uma s√©rie temporal. A ideia b√°sica √© aplicar uma m√©dia exponencial ponderada aos dados, atribuindo mais peso √†s observa√ß√µes mais recentes e menos peso √†s observa√ß√µes mais antigas.

"""

exponentialDecayWeightedAverage = indexedDataset_logScale.ewm(halflife=12, min_periods=0, adjust=True).mean()
plt.plot(indexedDataset_logScale)
plt.plot(exponentialDecayWeightedAverage, color='red')

# Subtraindo o dataset em escala logar√≠tma pelo decaimento exponencial
datasetLogScaleMinusExponentialMovingAverage = indexedDataset_logScale - exponentialDecayWeightedAverage
test_stationarity(datasetLogScaleMinusExponentialMovingAverage)

"""### 3¬∫ teste : Diferencia√ß√£o



---



**Objetivo:** Remover a tend√™ncia e tornar a s√©rie mais estacion√°ria.
"""

datasetLogDiffShifting = indexedDataset_logScale - indexedDataset_logScale.shift() #diferen√ßa entre o valor anterior e o atual
plt.plot(datasetLogDiffShifting)

datasetLogDiffShifting.dropna(inplace=True)
test_stationarity(datasetLogDiffShifting)

"""# Teste de correla√ß√£o parcial

Como definimos numa primeira tentativa o par√¢metro **P (lags: valores auto correlacionados)** e o par√¢metro **Q (tamanho de uma janela)** do ARIMA?

Vamos fazer isso com os **gr√°fico de ACF** (para ‚Äòq‚Äô) e o **gr√°fico de PACF** (para ‚Äòp‚Äô). Vamos selecionar como teste a base de dados da **diferencia√ß√£o**.

Vamos encontrar em **qual ponto cada gr√°fico passa em zero e este ponto ser√° o valor de P e Q inicial** (talvez em alguns casos pequenas altera√ß√µes nos par√¢metros do ARIMA possam melhorar/piorar os resultados, vale a pena alterar um pouco os valores um pouco positivamente e negativamente para olhar o desempenho).

Aplicando os plots ao nosso dataset de entrada (lags √© o n√∫mero de amostras):
"""

#ACF & PACF plots

lag_acf = acf(datasetLogDiffShifting, nlags=20)
lag_pacf = pacf(datasetLogDiffShifting, nlags=20, method='ols')

#Plot ACF:
plt.subplot(121)
plt.plot(lag_acf)
plt.axhline(y=0, linestyle='--', color='gray')
plt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')
plt.axhline(y=1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')
plt.title('Autocorrelation Function')

#Plot PACF
plt.subplot(122)
plt.plot(lag_pacf)
plt.axhline(y=0, linestyle='--', color='gray')
plt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')
plt.axhline(y=1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')
plt.title('Partial Autocorrelation Function')

plt.tight_layout()

#AR Model
model = ARIMA(datasetLogDiffShifting, order=(2,1,2))#(p,d,q)
results_AR = model.fit()
plt.plot(datasetLogDiffShifting)
plt.plot(results_AR.fittedvalues, color='red')
plt.title('RSS: %.4f'%sum((results_AR.fittedvalues - datasetLogDiffShifting['Close'])**2))
print('Plotting AR model')

"""**Validando o modelo com MAPE  (Mean Absolute Percentage Error)**"""

from sklearn.metrics import mean_absolute_error

# Obtenha as previs√µes
predictions = results_AR.fittedvalues

# Ajuste os √≠ndices para garantir correspond√™ncia
predictions.index = datasetLogDiffShifting.index

# Inverta a diferencia√ß√£o
# (indexedDataset_logScale['Close'].iloc[0]) para inverter a diferencia√ß√£o.
# np.cumsum(predictions) √© usado para calcular a soma cumulativa das previs√µes.
predicted_values = indexedDataset_logScale['Close'].iloc[0] + np.cumsum(predictions)

# Calcule o MAPE
mape = mean_absolute_error(datasetLogDiffShifting['Close'], predicted_values) * 100

print(f"MAPE: {mape:.2f}%")

"""# Testando o modelo com prophet üîÆ

"""

symbol = 'BZ=F'

start_date = '1987-05-20'
end_date = '2024-05-20'

# Use a fun√ß√£o download para obter os dados
df = yf.download(symbol, start=start_date, end=end_date)
df = df.reset_index('Date')
df['Date'] = pd.to_datetime(df['Date']) #realizando a convers√£o da data para formato datetime
df.drop(columns=['Open', 'High', 'Low', 'Volume', 'Adj Close'], inplace=True)
df[['ds','y']] = df[['Date','Close']]
df.head()

df_copy = df

#Separando em Treino e Teste

train_data = df.sample(frac=0.8, random_state=0)
test_data = df.drop(train_data.index)
print(f'training data size : {train_data.shape}')
print(f'testing data size : {test_data.shape}')

modelo = Prophet(daily_seasonality=True)
modelo.fit(train_data)
dataFramefuture = modelo.make_future_dataframe(periods=20, freq='M')
previsao = modelo.predict(dataFramefuture)
previsao.head()

modelo.plot(previsao, figsize=(20,6));
plt.plot(test_data['ds'], test_data['y'], '.r')

modelo.plot_components(previsao, figsize=(10,6));

# Extrair as colunas relevantes dos DataFrames
previsao_cols = ['ds', 'yhat']
valores_reais_cols = ['ds', 'y']

previsao = previsao[previsao_cols]
valores_reais = train_data[valores_reais_cols]

# Mesclar os DataFrames nas colunas 'ds' para comparar previs√µes e valores reais
resultados = pd.merge(previsao, valores_reais, on='ds', how='inner')

# Calcular o erro percentual absoluto para cada ponto de dados
resultados['erro_percentual_absoluto'] = np.abs((resultados['y'] - resultados['yhat']) / resultados['y']) * 100

# Calcular o MAPE
mape = np.mean(resultados['erro_percentual_absoluto'])

print(f"MAPE: {mape:.2f}%")

from prophet.diagnostics import cross_validation

df_cv = cross_validation(modelo, initial='730 days', period='180 days', horizon = '365 days')

df_cv.head()

from prophet.diagnostics import performance_metrics
df_p = performance_metrics(df_cv)
df_p

### Aplicando o algoritmo LSTM para prever

df = df_brent
df['Date'] = pd.to_datetime(df['Date']) #realizando a convers√£o da data para formato datetime
df.drop(columns=['Open', 'High', 'Low', 'Volume', 'Adj Close'], inplace=True)

df.head()

df.tail()

close_data = df['Close'].values
close_data = close_data.reshape(-1,1) #transformar em array

#Normalizando os dados

scaler = MinMaxScaler(feature_range=(0, 1))
scaler = scaler.fit(close_data)
close_data = scaler.transform(close_data)

#Separando em treino e teste

split_percent = 0.80
split = int(split_percent*len(close_data))

close_train = close_data[:split]
close_test = close_data[split:]

date_train = df['Date'][:split]
date_test = df['Date'][split:]

print(len(close_train))
print(len(close_test))

# Gerar sequ√™ncias temporais para treinamento e teste em um modelo de aprendizado de m√°quina

look_back = 10

train_generator = TimeseriesGenerator(close_train, close_train, length=look_back, batch_size=20)
test_generator = TimeseriesGenerator(close_test, close_test, length=look_back, batch_size=1)

from tensorflow.keras.metrics import MeanSquaredError

np.random.seed(7)

model = Sequential()
model.add(LSTM(100, activation='relu', input_shape=(look_back,1)))
model.add(Dense(1)),

model.compile(optimizer='adam', loss='mse', metrics=[MeanSquaredError()])

num_epochs = 20
model.fit(train_generator, epochs=num_epochs, verbose=1)

symbol = 'BZ=F'

start_date = '1987-05-20' #data mais antiga do valor do IPEA (url providenciada no challenge)
end_date = '2024-05-20' #data escolhida para o final dos dados

# Use a fun√ß√£o download para obter os dados
dados_brent = yf.download(symbol, start=start_date, end=end_date)


# Pr√©-processamento dos dados para o modelo Prophet
dados_prophet = dados_brent.rename(columns={'Data': 'ds', '√öltimo': 'y'})

# Dividir os dados em conjuntos de treinamento e teste para o Prophet
tamanho_treino = int(len(dados_prophet) * 0.8)
dados_treino_prophet = dados_prophet[:tamanho_treino]
dados_teste_prophet = dados_prophet[tamanho_treino:]

dados_prophet

dados_brent

dados_treino_prophet.head()

df.head()

dados_treino_prophet

dados_treino_prophet.reset_index(inplace=True)

dados_treino_prophet

dados_treino_prophet = dados_treino_prophet.rename(columns={'Date': 'ds'})
dados_treino_prophet = dados_treino_prophet.rename(columns={'Close': 'y'})

dados_treino_prophet

# Criar e treinar o modelo Prophet
modelo_prophet = Prophet(daily_seasonality=True)
modelo_prophet.fit(dados_treino_prophet)

# Realizar previs√µes com o modelo Prophet
datas_futuras_prophet = modelo_prophet.make_future_dataframe(periods=len(dados_teste_prophet), freq='D')
previsoes_prophet = modelo_prophet.predict(datas_futuras_prophet)

# Pr√©-processamento dos dados para o modelo LSTM
scaler = MinMaxScaler(feature_range=(0, 1))
dados_lstm = df[['Close']]
dados_lstm_scaled = scaler.fit_transform(dados_lstm)

# Criar conjunto de treinamento para LSTM
passos = 60
x_lstm, y_lstm = [], []
for i in range(len(dados_lstm_scaled)-passos-1):
    x_lstm.append(dados_lstm_scaled[i:(i+passos), 0])
    y_lstm.append(dados_lstm_scaled[i + passos, 0])
x_lstm, y_lstm = np.array(x_lstm), np.array(y_lstm)
x_lstm = np.reshape(x_lstm, (x_lstm.shape[0], x_lstm.shape[1], 1))

# Criar modelo LSTM
modelo_lstm = Sequential()
modelo_lstm.add(LSTM(units=50, return_sequences=True, input_shape=(x_lstm.shape[1], 1)))
modelo_lstm.add(LSTM(units=50))
modelo_lstm.add(Dense(units=1))
modelo_lstm.compile(optimizer='adam', loss='mean_squared_error')

# Treinar o modelo LSTM
modelo_lstm.fit(x_lstm, y_lstm, epochs=1, batch_size=1)

# Prever o fechamento para o dia xx/01/2024 usando ambos os modelos
data_previsao = pd.to_datetime('2024-05-04')
data_prophet = pd.DataFrame({'ds': [data_previsao]})
data_lstm = dados_lstm_scaled[-passos:].reshape(1, -1, 1)

# Previs√£o com o modelo Prophet
previsao_prophet = modelo_prophet.predict(data_prophet)

# Previs√£o com o modelo LSTM
previsao_lstm_scaled = modelo_lstm.predict(data_lstm)
previsao_lstm = scaler.inverse_transform(previsao_lstm_scaled)

# Imprimir as previs√µes
print(f'Previs√£o de fechamento para 04/05/2024 (Prophet): {previsao_prophet["yhat"].values[0]:.2f}')
print(f'Previs√£o de fechamento para 04/05/2024 (LSTM): {previsao_lstm[0, 0]:.2f}')

"""03/05/2024	83,60"""